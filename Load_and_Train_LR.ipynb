{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.cloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-826164022f39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# from pyspark.mllib.tree import GradientBoostedTrees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.cloud'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.sql.functions import udf, round, col\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, VectorAssembler, CountVectorizer, IDF, IDFModel, CountVectorizerModel, StringIndexer\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "from google.cloud import storage\n",
    "\n",
    "# from pyspark.mllib.tree import GradientBoostedTrees\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from io import BytesIO\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from pyspark.mllib.linalg import Vector as MLLibVector, Vectors as MLLibVectors\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "full_data = spark.read.json('gs://bdl_project/yelp.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = full_data.limit(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'bdl_project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and applying all pre-trained models\n",
    "indexer = PipelineModel.load('gs://' + bucket + '/indexer')\n",
    "raw_data = indexer.transform(raw_data)\n",
    "raw_data = raw_data.drop('business_id', 'user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process = PipelineModel.load('gs://' + bucket + '/pre_process')\n",
    "data = pre_process.transform(raw_data)\n",
    "data = data.drop('text', 'tokenized_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stemming\n",
    "ps = PorterStemmer()\n",
    "stemmer = udf(lambda text: [ps.stem(token) for token in text], ArrayType(StringType()))\n",
    "data = data.withColumn('stemmed_text', stemmer('filtered_text'))\n",
    "data = data.drop('filtered_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train test split\n",
    "#(train, test) = data.randomSplit([0.05, 0.95], seed=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model = CountVectorizerModel().load('gs://' + bucket + '/cv_model')  \n",
    "cvData = cv_model.transform(data)\n",
    "idf_model = IDFModel().load('gs://' + bucket + '/idf_model')\n",
    "tfidfData = idf_model.transform(cvData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cool',\n",
       " 'date',\n",
       " 'funny',\n",
       " 'review_id',\n",
       " 'stars',\n",
       " 'useful',\n",
       " 'business_ind',\n",
       " 'user_ind',\n",
       " 'stemmed_text',\n",
       " 'tf',\n",
       " 'tf_idf']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = BytesIO(file_io.read_file_to_string('gs://bdl_project/glove.npy', binary_mode=True))\n",
    "glove = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocabulary and word index\n",
    "vocab = cv_model.vocabulary\n",
    "ind2word = dict(zip(range(len(vocab)), vocab))\n",
    "vocab_len = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262144"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain glove stacking for the words in the vocabulary\n",
    "\n",
    "# for i in range(vocab_len):\n",
    "#     if i ==0:\n",
    "#         glove = embedding_glove[vocab[i]].numpy()\n",
    "#     else:\n",
    "#         glove = np.vstack((glove, embedding_glove[vocab[i]].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering to obtain sentence representation using weighted glove representations\n",
    "glove_cols = tfidfData.rdd.map(lambda x:(x[0], x[1], x[2], x[3], x[4], x[5], x[6], x[7], x[8], x[9], Vectors.dense(x[10].dot(glove)/x[10].dot(np.ones(vocab_len))))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming glove column names with original column names\n",
    "oldColumns = glove_cols.schema.names\n",
    "newColumns = tfidfData.schema.names[:-1] + [\"glove_emb\"]\n",
    "\n",
    "train = reduce(lambda glove_cols, idx: glove_cols.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(oldColumns)), glove_cols)\n",
    "train = train.drop(\"lemmatized_text\", \"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'ch16b033'\n",
    "min_date = datetime.now()\n",
    "client = storage.Client()\n",
    "bucket_obj = client.get_bucket(bucket)\n",
    "blob = bucket_obj.blob('min_date.txt')\n",
    "blob.upload_from_string(str(min_date))\n",
    "\n",
    "delta_days = udf(lambda x: int((min_date-datetime.strptime(x, '%Y-%m-%d %H:%M:%S')).days))\n",
    "train = train.withColumn('delta_days', delta_days('date'))\n",
    "train = train.drop('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "train = train.withColumn(\"delta_days\",train[\"delta_days\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cool: long (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- business_ind: double (nullable = true)\n",
      " |-- user_ind: double (nullable = true)\n",
      " |-- stemmed_text: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- glove_emb: vector (nullable = true)\n",
      " |-- delta_days: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_store = 'ch16b033'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = ['cool','funny','useful','glove_emb','delta_days','business_ind','user_ind'], outputCol=\"features\")\n",
    "train = assembler.transform(train).select(\"features\", \"stars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_train = train.rdd.map(lambda x: LabeledPoint(x.stars, MLLibVectors.fromML(x.features))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.withColumnRenamed('stars', 'label')\n",
    "lr = LogisticRegression(maxIter=3, regParam=0.3, elasticNetParam=0.8)\n",
    "model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('gs://' + bucket_store + '/logreg_model')\n",
    "print('Trained and saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = model.summary\n",
    "accuracy = trainingSummary.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy =', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbassembler = VectorAssembler(inputCols = ['cool','funny','useful','glove_emb','delta_days','business_ind','user_ind'], outputCol=\"features\")\n",
    "train = xgbassembler.transform(train).select(\"features\", \"stars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input to xgboost needs to be an RDD of LabeledPoints and the feature vector needs to be of type mllib.linalg.vector\n",
    "xgb_train = train.rdd.map(lambda x: LabeledPoint(x.stars, MLLibVectors.fromML(x.features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categoricalFeaturesInfo is a dict with key as column index of categorical variable and value as no. of classes\n",
    "num_bus = 209393\n",
    "num_users = 1968703\n",
    "model = GradientBoostedTrees.trainRegressor(xgb_train, categoricalFeaturesInfo={104:num_bus, 105:num_users}, numIterations=3)\n",
    "print('Model trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('gs://nithya1998/xgb_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = model.predict(xgb_train.map(lambda x: x.features))\n",
    "stars_and_pred = train.rdd.map(lambda lp: lp.stars).zip(train_predictions).toDF()\n",
    "stars_and_pred = stars_and_pred.withColumn('pred', round(col('_2'))).withColumnRenamed('_1', 'stars')\n",
    "stars_and_pred = stars_and_pred.select('stars','pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_multi = MulticlassMetrics(stars_and_pred.rdd.map(tuple))\n",
    "acc = metrics_multi.accuracy\n",
    "print(\"Training Accuracy =\", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
